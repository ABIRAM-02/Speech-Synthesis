from nltk.tokenize import word_tokenize
with open ('LasVegasFeb21_2020.txt') as fin, open('token15.txt','w') as fout:
for line in fin:
    tokens = word_tokenize(line)
    print(' '.join(tokens), end='\n', file=fout)
